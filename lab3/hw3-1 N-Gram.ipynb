{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn import preprocessing\n",
    "import string\n",
    "import re\n",
    "from copy import deepcopy\n",
    "\n",
    "stopwords = map(unicode, set(set(stopwords.words('english')) - set([unicode('not'), unicode('no')])))\n",
    "lematzr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/questions/771918/how-do-i-do-word-stemming-or-lemmatization\n",
    "#  Lematizing, removing punctuation and removing caps. Also removing stopwords but keeping negatives like no and not\n",
    "# Not adding words that are causing unicode errors: fiancé, café, crêpe, puréed, québec, \n",
    "# clichés, clichés, aurvåg, problemsthe, clichés, seeing",
    "      \n",
    "def remove_punc(s):\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    return regex.sub(' ', s)\n",
    "\n",
    "def decap(s):\n",
    "    return s.lower()\n",
    "\n",
    "def lemmatize_word(lmtr, s):\n",
    "    return lmtr.lemmatize(s)\n",
    "\n",
    "def two_gram(input_list):\n",
    "    return zip(input_list, input_list[1:])\n",
    "\n",
    "def prepro(s):\n",
    "    string = remove_punc(decap(s)).split()\n",
    "    formated = []\n",
    "    for word in string:\n",
    "            try:\n",
    "                if unicode(word) not in stopwords:\n",
    "                    formated.append(str(lemmatize_word(lematzr, unicode(word))))\n",
    "            except (UnicodeDecodeError, UnicodeEncodeError):\n",
    "\n",
    "                print \"unicode error for: \" + word\n",
    "    return two_gram(formated)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the data\n",
    "\n",
    "amazon = pd.read_csv(\"amazon_cells_labelled.txt\", sep=\"\\t\", header=None).dropna().reset_index().drop(\"index\", axis=1)\n",
    "yelp = pd.read_csv(\"yelp_labelled.txt\", sep=\"\\t\", header=None, encoding=\"utf-8\").dropna().reset_index().drop(\"index\", axis=1)\n",
    "imdb_file = open(\"imdb_labelled.txt\", \"r\")\n",
    "\n",
    "splits = []\n",
    "for line in imdb_file:\n",
    "    line = line.replace(\"\\n\",\"\")\n",
    "    splits.append(line.split(\"\\t\"))\n",
    "    \n",
    "imdb = pd.DataFrame(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unicode error for: fiancé\n",
      "unicode error for: café\n",
      "unicode error for: crêpe\n",
      "unicode error for: puréed\n",
      "unicode error for: \n",
      "unicode error for: québec\n",
      "unicode error for: is",
      "was\n",
      "unicode error for: \n",
      "unicode error for: clichés\n",
      "unicode error for: clichés\n",
      "unicode error for: aurvåg\n",
      "unicode error for: \n",
      "unicode error for: problemsthe\n",
      "unicode error for: \n",
      "unicode error for: clichés\n",
      "unicode error for: \n",
      "unicode error for: seeing",
      "\n"
     ]
    }
   ],
   "source": [
    "amazon[0] = amazon[0].map(prepro)\n",
    "yelp[0] = yelp[0].map(prepro)\n",
    "imdb[0] = imdb[0].map(prepro)\n",
    "imdb[1] = imdb[1].map(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amazon_train_true = amazon[amazon[1] > 0].head(400)\n",
    "yelp_train_true = yelp[yelp[1] > 0].head(400)\n",
    "imdb_train_true = imdb[imdb[1] > 0].head(400)\n",
    "\n",
    "amazon_test_true = amazon[amazon[1] > 0].tail(100)\n",
    "yelp_test_true = yelp[yelp[1] > 0].tail(100)\n",
    "imdb_test_true = imdb[imdb[1] > 0].tail(100)\n",
    "\n",
    "amazon_train_false = amazon[amazon[1] == 0].head(400)\n",
    "yelp_train_false = yelp[yelp[1] == 0].head(400)\n",
    "imdb_train_false = imdb[imdb[1] == 0].head(400)\n",
    "\n",
    "amazon_test_false = amazon[amazon[1] == 0].tail(100)\n",
    "yelp_test_false = yelp[yelp[1] == 0].tail(100)\n",
    "imdb_test_false = imdb[imdb[1] == 0].tail(100)\n",
    "\n",
    "train_frames = [amazon_train_true, amazon_train_false, yelp_train_true, yelp_train_false, imdb_train_true, imdb_train_false]\n",
    "test_frames = [amazon_test_true, amazon_test_false, yelp_test_true, yelp_test_false, imdb_test_true, imdb_test_false]\n",
    "\n",
    "train = pd.concat(train_frames).reset_index().drop(\"index\", axis=1)\n",
    "train.columns = [\"words\",\"score\"]\n",
    "test = pd.concat(test_frames).reset_index().drop(\"index\", axis=1)\n",
    "test.columns = [\"words\", \"score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bag = {}\n",
    "def create_bag(word_lst):\n",
    "    for word in word_lst:\n",
    "        bag[word] = 0.0\n",
    "    return bag\n",
    "\n",
    "def index_bag(bag):\n",
    "    indx = 0\n",
    "    for key in bag.keys():\n",
    "        bag[key] = indx\n",
    "        indx += 1\n",
    "    return bag\n",
    "\n",
    "def count(word_lst):\n",
    "    zeros = [0 for x in range(0,len(bag))]\n",
    "    \n",
    "    for word in word_lst:\n",
    "        if word in bag.keys():    \n",
    "            zeros[int(bag[word])] += 1.0\n",
    "\n",
    "    return zeros\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the bag of words from TRAINING DATA ONLY!!!\n",
    "train.words.map(create_bag);\n",
    "index_bag(bag);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize the dataset with l1. See page 611 of HTF\n",
    "vects_train = pd.DataFrame(map(count, train.words.tolist()), columns=bag.keys())\n",
    "vects_test = pd.DataFrame(map(count, test.words.tolist()), columns=bag.keys())\n",
    "\n",
    "train_normalized = pd.DataFrame(preprocessing.normalize(vects_train, norm='l1'))\n",
    "test_normalized = pd.DataFrame(preprocessing.normalize(vects_test, norm='l1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorized_train = pd.concat([train, train_normalized], axis=1);\n",
    "vectorized_test = pd.concat([test, test_normalized], axis=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def first_means():\n",
    "    import random\n",
    "    means = []\n",
    "    indexes = range(0,len(train_normalized))\n",
    "    mean_idxs = random.sample(indexes, k)\n",
    "    for i in mean_idxs:\n",
    "        means.append(train_normalized.iloc[i].values)\n",
    "    return means\n",
    "\n",
    "def assign_to_clusters(means):\n",
    "    clusters = []\n",
    "    for i in range(0,k):\n",
    "        clusters.append([])\n",
    "\n",
    "    for i in indexes:\n",
    "        distances = []\n",
    "        for j in means:\n",
    "            distances.append(np.linalg.norm(j - train_normalized.iloc[i]))\n",
    "        cluster_choice = np.argmin(distances)\n",
    "        clusters[cluster_choice].append(i) \n",
    "    return clusters\n",
    "\n",
    "def new_means(clusters):\n",
    "    means = []\n",
    "    for cluster in clusters:\n",
    "        means.append(np.mean(train_normalized.iloc[cluster]))\n",
    "    return means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexes = range(0,len(train_normalized))\n",
    "\n",
    "def k_means(k, train_normalized):\n",
    "    \n",
    "    means = first_means()\n",
    "    iterations = 0\n",
    "    original_clusters = assign_to_clusters(means)\n",
    "    means = new_means(original_clusters)\n",
    "    new_clusters = assign_to_clusters(means)\n",
    "    means = new_means(new_clusters)\n",
    "    \n",
    "    while original_clusters != new_clusters:\n",
    "        original_clusters = assign_to_clusters(means)\n",
    "        means = new_means(original_clusters)\n",
    "        new_clusters = assign_to_clusters(means)\n",
    "        means = new_means(new_clusters)\n",
    "        iterations += 1\n",
    "    \n",
    "    print \"iterations: \" + str(iterations)\n",
    "    \n",
    "    return new_clusters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations: 0\n"
     ]
    }
   ],
   "source": [
    "k=2\n",
    "clustered = k_means(k, train_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of cluster: 2399\n",
      "number of ones in cluster 0: 1199.0\n",
      "percent of 1s in 0 cluster: 49.9791579825\n",
      "Length of cluster: 1\n",
      "number of ones in cluster 1: 1.0\n",
      "percent of 1s in 1 cluster: 100.0\n"
     ]
    }
   ],
   "source": [
    "for cluster in clustered:\n",
    "    the_cluster = cluster\n",
    "\n",
    "    cluster_score = str(int(round(train.score.iloc[the_cluster].mean())))\n",
    "    print \"Length of cluster: \" + str(len(the_cluster))\n",
    "    print \"number of ones in cluster \"+ cluster_score + \": \" + str(train.score.iloc[the_cluster].mean()*len(the_cluster))\n",
    "    print \"percent of 1s in \" + cluster_score  + \" cluster: \" + str(train.score.iloc[the_cluster].mean() * 100)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "md = linear_model.LogisticRegression()\n",
    "est = md.fit(train_normalized, vectorized_train.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.636666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[269, 187],\n",
       "       [ 31, 113]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = est.score(test_normalized, vectorized_test.score)\n",
    "print score\n",
    "\n",
    "pred = est.predict(test_normalized)\n",
    "confusion_matrix(pred, vectorized_test.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1893 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-2be47193014f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1893\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 1893 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1893"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag[('work', 'great')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
