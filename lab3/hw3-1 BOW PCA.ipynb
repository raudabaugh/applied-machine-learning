{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn import preprocessing\n",
    "import string\n",
    "import scipy\n",
    "import re\n",
    "from copy import deepcopy\n",
    "\n",
    "stopwords = map(unicode, set(set(stopwords.words('english')) - set([unicode('not'), unicode('no')])))\n",
    "lematzr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/questions/771918/how-do-i-do-word-stemming-or-lemmatization\n",
    "#  Lematizing, removing punctuation and removing caps. Also removing stopwords but keeping negatives like no and not\n",
    "# Not adding words that are causing unicode errors: fiancé, café, crêpe, puréed, québec, \n",
    "# clichés, clichés, aurvåg, problemsthe, clichés, seeing",
    "      \n",
    "def remove_punc(s):\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    return regex.sub(' ', s)\n",
    "\n",
    "def decap(s):\n",
    "    return s.lower()\n",
    "\n",
    "def lemmatize_word(lmtr, s):\n",
    "    return lmtr.lemmatize(s)\n",
    "\n",
    "def prepro(s):\n",
    "    string = remove_punc(decap(s)).split()\n",
    "    formated = []\n",
    "    for word in string:\n",
    "\n",
    "            try:\n",
    "                if unicode(word) not in stopwords:\n",
    "                    formated.append(str(lemmatize_word(lematzr, unicode(word))))\n",
    "            except (UnicodeDecodeError, UnicodeEncodeError):\n",
    "\n",
    "                print \"unicode error for: \" + word\n",
    "    return formated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the data\n",
    "\n",
    "amazon = pd.read_csv(\"amazon_cells_labelled.txt\", sep=\"\\t\", header=None).dropna().reset_index().drop(\"index\", axis=1)\n",
    "yelp = pd.read_csv(\"yelp_labelled.txt\", sep=\"\\t\", header=None, encoding=\"utf-8\").dropna().reset_index().drop(\"index\", axis=1)\n",
    "imdb_file = open(\"imdb_labelled.txt\", \"r\")\n",
    "\n",
    "splits = []\n",
    "for line in imdb_file:\n",
    "    line = line.replace(\"\\n\",\"\")\n",
    "    splits.append(line.split(\"\\t\"))\n",
    "    \n",
    "imdb = pd.DataFrame(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unicode error for: fiancé\n",
      "unicode error for: café\n",
      "unicode error for: crêpe\n",
      "unicode error for: puréed\n",
      "unicode error for: \n",
      "unicode error for: québec\n",
      "unicode error for: is",
      "was\n",
      "unicode error for: \n",
      "unicode error for: clichés\n",
      "unicode error for: clichés\n",
      "unicode error for: aurvåg\n",
      "unicode error for: \n",
      "unicode error for: problemsthe\n",
      "unicode error for: \n",
      "unicode error for: clichés\n",
      "unicode error for: \n",
      "unicode error for: seeing",
      "\n"
     ]
    }
   ],
   "source": [
    "amazon[0] = amazon[0].map(prepro)\n",
    "yelp[0] = yelp[0].map(prepro)\n",
    "imdb[0] = imdb[0].map(prepro)\n",
    "imdb[1] = imdb[1].map(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amazon_train_true = amazon[amazon[1] > 0].head(400)\n",
    "yelp_train_true = yelp[yelp[1] > 0].head(400)\n",
    "imdb_train_true = imdb[imdb[1] > 0].head(400)\n",
    "\n",
    "amazon_test_true = amazon[amazon[1] > 0].tail(100)\n",
    "yelp_test_true = yelp[yelp[1] > 0].tail(100)\n",
    "imdb_test_true = imdb[imdb[1] > 0].tail(100)\n",
    "\n",
    "amazon_train_false = amazon[amazon[1] == 0].head(400)\n",
    "yelp_train_false = yelp[yelp[1] == 0].head(400)\n",
    "imdb_train_false = imdb[imdb[1] == 0].head(400)\n",
    "\n",
    "amazon_test_false = amazon[amazon[1] == 0].tail(100)\n",
    "yelp_test_false = yelp[yelp[1] == 0].tail(100)\n",
    "imdb_test_false = imdb[imdb[1] == 0].tail(100)\n",
    "\n",
    "train_frames = [amazon_train_true, amazon_train_false, yelp_train_true, yelp_train_false, imdb_train_true, imdb_train_false]\n",
    "test_frames = [amazon_test_true, amazon_test_false, yelp_test_true, yelp_test_false, imdb_test_true, imdb_test_false]\n",
    "\n",
    "train = pd.concat(train_frames).reset_index().drop(\"index\", axis=1)\n",
    "train.columns = [\"words\",\"score\"]\n",
    "test = pd.concat(test_frames).reset_index().drop(\"index\", axis=1)\n",
    "test.columns = [\"words\", \"score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bag = {}\n",
    "def create_bag(word_lst):\n",
    "    for word in word_lst:\n",
    "        bag[word] = 0.0\n",
    "    return bag\n",
    "\n",
    "def index_bag(bag):\n",
    "    indx = 0\n",
    "    for key in bag.keys():\n",
    "        bag[key] = indx\n",
    "        indx += 1\n",
    "    return bag\n",
    "\n",
    "def count(word_lst):\n",
    "    zeros = [0 for x in range(0,len(bag))]\n",
    "    \n",
    "    for word in word_lst:\n",
    "        if word in bag.keys():    \n",
    "            zeros[int(bag[word])] += 1.0\n",
    "\n",
    "    return zeros\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the bag of words from TRAINING DATA ONLY!!!\n",
    "train.words.map(create_bag);\n",
    "index_bag(bag);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize the dataset with l1. See page 611 of HTF\n",
    "vects_train = pd.DataFrame(map(count, train.words.tolist()), columns=bag.keys())\n",
    "vects_test = pd.DataFrame(map(count, test.words.tolist()), columns=bag.keys())\n",
    "\n",
    "train_normalized = pd.DataFrame(preprocessing.normalize(vects_train, norm='l1'))\n",
    "train_mean = train_normalized.mean()\n",
    "test_normalized = pd.DataFrame(preprocessing.normalize(vects_test, norm='l1'))\n",
    "test_mean = test_normalized.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_centered = train_normalized.apply(lambda x: x - train_mean, axis=1)\n",
    "test_centered = test_normalized.apply(lambda x: x - test_mean, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "U, s, V = scipy.linalg.svd(train_centered, full_matrices=False)\n",
    "U_test, s_test, V_test = scipy.linalg.svd(test_centered, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_train_50 = np.concatenate((s[0:50], [0]*(len(s)-50)), axis=1)\n",
    "s_train_100 = np.concatenate((s[0:100], [0]*(len(s)-100)), axis=1)\n",
    "s_train_150 = np.concatenate((s[0:150], [0]*(len(s)-150)), axis=1)\n",
    "\n",
    "S_train_50 = np.diag(s_50)\n",
    "S_train_100 = np.diag(s_100)\n",
    "S_train_150 = np.diag(s_150)\n",
    "\n",
    "\n",
    "s_test_50 = np.concatenate((s_test[0:50], [0]*(len(s_test)-50)), axis=1)\n",
    "s_test_100 = np.concatenate((s_test[0:100], [0]*(len(s_test)-100)), axis=1)\n",
    "s_test_150 = np.concatenate((s_test[0:150], [0]*(len(s_test)-150)), axis=1)\n",
    "\n",
    "S_test_50 = np.diag(s_test_50)\n",
    "S_test_100 = np.diag(s_test_100)\n",
    "S_test_150 = np.diag(s_test_150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_50 = pd.DataFrame(np.dot(U, np.dot(S_50, V)))\n",
    "train_100 = pd.DataFrame(np.dot(U, np.dot(S_100, V)))\n",
    "train_150 = pd.DataFrame(np.dot(U, np.dot(S_150, V)))\n",
    "\n",
    "test_50 = pd.DataFrame(np.dot(U_test, np.dot(S_test_50, V_test)))\n",
    "test_100 = pd.DataFrame(np.dot(U_test, np.dot(S_test_100, V_test)))\n",
    "test_150 = pd.DataFrame(np.dot(U_test, np.dot(S_test_150, V_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations: 3\n",
      "Length of cluster: 2373\n",
      "number of ones in cluster 0: 1175.0\n",
      "percent of 1s in 0 cluster: 49.5153813738\n",
      "Length of cluster: 27\n",
      "number of ones in cluster 1: 25.0\n",
      "percent of 1s in 1 cluster: 92.5925925926\n",
      "Regression score: 0.645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[243, 156],\n",
       "       [ 57, 144]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 50\n",
    "\n",
    "k = 2\n",
    "train_normalized = train_50\n",
    "test_normalized = test_50\n",
    "\n",
    "vectorized_train = pd.concat([train, train_normalized], axis=1);\n",
    "vectorized_test = pd.concat([test, test_normalized], axis=1);\n",
    "\n",
    "indexes = range(0,len(train_normalized))\n",
    "\n",
    "def first_means():\n",
    "    import random\n",
    "    means = []\n",
    "    indexes = range(0,len(train_normalized))\n",
    "    mean_idxs = random.sample(indexes, k)\n",
    "    for i in mean_idxs:\n",
    "        means.append(train_normalized.iloc[i].values)\n",
    "    return means\n",
    "\n",
    "def assign_to_clusters(means):\n",
    "    clusters = []\n",
    "    for i in range(0,k):\n",
    "        clusters.append([])\n",
    "\n",
    "    for i in indexes:\n",
    "        distances = []\n",
    "        for j in means:\n",
    "            distances.append(np.linalg.norm(j - train_normalized.iloc[i]))\n",
    "        cluster_choice = np.argmin(distances)\n",
    "        clusters[cluster_choice].append(i) \n",
    "    return clusters\n",
    "\n",
    "def new_means(clusters):\n",
    "    means = []\n",
    "    for cluster in clusters:\n",
    "        means.append(np.mean(train_normalized.iloc[cluster]))\n",
    "    return means\n",
    "\n",
    "def k_means(k, train_normalized):\n",
    "    means = first_means()\n",
    "    iterations = 0\n",
    "    original_clusters = assign_to_clusters(means)\n",
    "    means = new_means(original_clusters)\n",
    "    new_clusters = assign_to_clusters(means)\n",
    "    means = new_means(new_clusters)\n",
    "    \n",
    "    while original_clusters != new_clusters:\n",
    "        original_clusters = assign_to_clusters(means)\n",
    "        means = new_means(original_clusters)\n",
    "        new_clusters = assign_to_clusters(means)\n",
    "        means = new_means(new_clusters)\n",
    "        iterations += 1\n",
    "    \n",
    "    print \"iterations: \" + str(iterations)\n",
    "    \n",
    "    return new_clusters\n",
    "\n",
    "clustered = k_means(2, train_normalized)\n",
    "\n",
    "for cluster in clustered:\n",
    "    the_cluster = cluster\n",
    "\n",
    "    cluster_score = str(int(round(train.score.iloc[the_cluster].mean())))\n",
    "    print \"Length of cluster: \" + str(len(the_cluster))\n",
    "    print \"number of ones in cluster \"+ cluster_score + \": \" + str(train.score.iloc[the_cluster].mean()*len(the_cluster))\n",
    "    print \"percent of 1s in \" + cluster_score  + \" cluster: \" + str(train.score.iloc[the_cluster].mean() * 100)    \n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "md = linear_model.LogisticRegression()\n",
    "est = md.fit(train_normalized, vectorized_train.score)\n",
    "\n",
    "score = est.score(test_normalized, vectorized_test.score)\n",
    "print \"Regression score: \" + str(score)\n",
    "\n",
    "pred = est.predict(test_normalized)\n",
    "confusion_matrix(pred, vectorized_test.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations: 2\n",
      "Length of cluster: 98\n",
      "number of ones in cluster 1: 96.0\n",
      "percent of 1s in 1 cluster: 97.9591836735\n",
      "Length of cluster: 2302\n",
      "number of ones in cluster 0: 1104.0\n",
      "percent of 1s in 0 cluster: 47.9582971329\n",
      "0.691666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[240, 125],\n",
       "       [ 60, 175]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100\n",
    "\n",
    "k = 2\n",
    "train_normalized = train_100\n",
    "test_normalized = test_100\n",
    "\n",
    "vectorized_train = pd.concat([train, train_normalized], axis=1);\n",
    "vectorized_test = pd.concat([test, test_normalized], axis=1);\n",
    "\n",
    "indexes = range(0,len(train_normalized))\n",
    "\n",
    "def first_means():\n",
    "    import random\n",
    "    means = []\n",
    "    indexes = range(0,len(train_normalized))\n",
    "    mean_idxs = random.sample(indexes, k)\n",
    "    for i in mean_idxs:\n",
    "        means.append(train_normalized.iloc[i].values)\n",
    "    return means\n",
    "\n",
    "def assign_to_clusters(means):\n",
    "    clusters = []\n",
    "    for i in range(0,k):\n",
    "        clusters.append([])\n",
    "\n",
    "    for i in indexes:\n",
    "        distances = []\n",
    "        for j in means:\n",
    "            distances.append(np.linalg.norm(j - train_normalized.iloc[i]))\n",
    "        cluster_choice = np.argmin(distances)\n",
    "        clusters[cluster_choice].append(i) \n",
    "    return clusters\n",
    "\n",
    "def new_means(clusters):\n",
    "    means = []\n",
    "    for cluster in clusters:\n",
    "        means.append(np.mean(train_normalized.iloc[cluster]))\n",
    "    return means\n",
    "\n",
    "def k_means(k, train_normalized):\n",
    "    means = first_means()\n",
    "    iterations = 0\n",
    "    original_clusters = assign_to_clusters(means)\n",
    "    means = new_means(original_clusters)\n",
    "    new_clusters = assign_to_clusters(means)\n",
    "    means = new_means(new_clusters)\n",
    "    \n",
    "    while original_clusters != new_clusters:\n",
    "        original_clusters = assign_to_clusters(means)\n",
    "        means = new_means(original_clusters)\n",
    "        new_clusters = assign_to_clusters(means)\n",
    "        means = new_means(new_clusters)\n",
    "        iterations += 1\n",
    "    \n",
    "    print \"iterations: \" + str(iterations)\n",
    "    \n",
    "    return new_clusters\n",
    "\n",
    "clustered = k_means(2, train_normalized)\n",
    "\n",
    "for cluster in clustered:\n",
    "    the_cluster = cluster\n",
    "\n",
    "    cluster_score = str(int(round(train.score.iloc[the_cluster].mean())))\n",
    "    print \"Length of cluster: \" + str(len(the_cluster))\n",
    "    print \"number of ones in cluster \"+ cluster_score + \": \" + str(train.score.iloc[the_cluster].mean()*len(the_cluster))\n",
    "    print \"percent of 1s in \" + cluster_score  + \" cluster: \" + str(train.score.iloc[the_cluster].mean() * 100)    \n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "md = linear_model.LogisticRegression()\n",
    "est = md.fit(train_normalized, vectorized_train.score)\n",
    "\n",
    "score = est.score(test_normalized, vectorized_test.score)\n",
    "print score\n",
    "\n",
    "pred = est.predict(test_normalized)\n",
    "confusion_matrix(pred, vectorized_test.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations: 0\n",
      "Length of cluster: 10\n",
      "number of ones in cluster 0: 0.0\n",
      "percent of 1s in 0 cluster: 0.0\n",
      "Length of cluster: 2390\n",
      "number of ones in cluster 1: 1200.0\n",
      "percent of 1s in 1 cluster: 50.2092050209\n",
      "0.746666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[251, 103],\n",
       "       [ 49, 197]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 150\n",
    "\n",
    "k = 2\n",
    "train_normalized = train_150\n",
    "test_normalized = test_150\n",
    "\n",
    "vectorized_train = pd.concat([train, train_normalized], axis=1);\n",
    "vectorized_test = pd.concat([test, test_normalized], axis=1);\n",
    "\n",
    "indexes = range(0,len(train_normalized))\n",
    "\n",
    "def first_means():\n",
    "    import random\n",
    "    means = []\n",
    "    indexes = range(0,len(train_normalized))\n",
    "    mean_idxs = random.sample(indexes, k)\n",
    "    for i in mean_idxs:\n",
    "        means.append(train_normalized.iloc[i].values)\n",
    "    return means\n",
    "\n",
    "def assign_to_clusters(means):\n",
    "    clusters = []\n",
    "    for i in range(0,k):\n",
    "        clusters.append([])\n",
    "\n",
    "    for i in indexes:\n",
    "        distances = []\n",
    "        for j in means:\n",
    "            distances.append(np.linalg.norm(j - train_normalized.iloc[i]))\n",
    "        cluster_choice = np.argmin(distances)\n",
    "        clusters[cluster_choice].append(i) \n",
    "    return clusters\n",
    "\n",
    "def new_means(clusters):\n",
    "    means = []\n",
    "    for cluster in clusters:\n",
    "        means.append(np.mean(train_normalized.iloc[cluster]))\n",
    "    return means\n",
    "\n",
    "def k_means(k, train_normalized):\n",
    "    means = first_means()\n",
    "    iterations = 0\n",
    "    original_clusters = assign_to_clusters(means)\n",
    "    means = new_means(original_clusters)\n",
    "    new_clusters = assign_to_clusters(means)\n",
    "    means = new_means(new_clusters)\n",
    "    \n",
    "    while original_clusters != new_clusters:\n",
    "        original_clusters = assign_to_clusters(means)\n",
    "        means = new_means(original_clusters)\n",
    "        new_clusters = assign_to_clusters(means)\n",
    "        means = new_means(new_clusters)\n",
    "        iterations += 1\n",
    "    \n",
    "    print \"iterations: \" + str(iterations)\n",
    "    \n",
    "    return new_clusters\n",
    "\n",
    "clustered = k_means(2, train_normalized)\n",
    "\n",
    "for cluster in clustered:\n",
    "    the_cluster = cluster\n",
    "\n",
    "    cluster_score = str(int(round(train.score.iloc[the_cluster].mean())))\n",
    "    print \"Length of cluster: \" + str(len(the_cluster))\n",
    "    print \"number of ones in cluster \"+ cluster_score + \": \" + str(train.score.iloc[the_cluster].mean()*len(the_cluster))\n",
    "    print \"percent of 1s in \" + cluster_score  + \" cluster: \" + str(train.score.iloc[the_cluster].mean() * 100)    \n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "md = linear_model.LogisticRegression()\n",
    "est = md.fit(train_normalized, vectorized_train.score)\n",
    "\n",
    "score = est.score(test_normalized, vectorized_test.score)\n",
    "print score\n",
    "\n",
    "pred = est.predict(test_normalized)\n",
    "confusion_matrix(pred, vectorized_test.score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
